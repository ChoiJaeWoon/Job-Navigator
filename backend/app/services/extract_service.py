# ì´ íŒŒì¼ì€ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ ì—”ì§„ì˜ ë¡œì§ì„ ë‹´ë‹¹
# ì‚¬ì „ ê¸°ë°˜(Tech_stack) + ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ë§¤ì¹­

import fitz # PDF ì¶”ì¶œì„ ìœ„í•œ PyMuPDF íŒ¨í‚¤ì§€
import re   # ì •ê·œí‘œí˜„ì‹ í™œìš©
import nltk # ë¬¸ì¥ ë¶„ë¦¬ë¥¼ ìœ„í•œ NLTK íŒ¨í‚¤ì§€
import numpy as np # ë²¡í„° ê³„ì‚°ì„ ìœ„í•œ numpy
from nltk.tokenize import sent_tokenize # ë¬¸ì¥ ë¶„ë¦¬
from sentence_transformers import SentenceTransformer # ì„ë² ë”© ëª¨ë¸
from sklearn.metrics.pairwise import cosine_similarity # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
from app.tech_stack import TECH_STACK # ê¸°ìˆ ìŠ¤íƒ ì‚¬ì „ ë¶ˆëŸ¬ì˜¤ê¸°
import traceback

# NLTKì—ì„œ punkt í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
nltk.download('punkt')
nltk.download('punkt_tab')

# PDF íŒŒì¼ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(pdf_path: str) -> str:
    """
    PDF ê²½ë¡œë¥¼ ë°›ì•„ì„œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    text = ""
    doc = fitz.open(pdf_path) # pdfê²½ë¡œë¥¼ ê°€ì ¸ì™€ì„œ ì—´ê¸°
    for page in doc: # í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë°˜ë³µ
        text += page.get_text() # í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    return text

# ì‚¬ì „ ê¸°ë°˜ ë§¤ì¹­ (ì •í™• ì¼ì¹˜ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­)
def dictionary_based_matching(text: str, tech_stack: list) -> list:
    """
    ì‚¬ì „ì— ì •ì˜ëœ ê¸°ìˆ ìŠ¤íƒ ë¦¬ìŠ¤íŠ¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    found = []
    for tech in tech_stack:
        # \bë¥¼ ì‚¬ìš©í•´ì„œ ë‹¨ì–´ ê²½ê³„ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ë§Œ ì¶”ì¶œ
        """
        ë¬¸ì¥:   "ë‚˜ëŠ” Pythonì„ ì‚¬ìš©í•©ë‹ˆë‹¤"
                 ^    ^         ^
        ë‹¨ì–´ê²½ê³„: \b  Python  \b
        """
        if re.search(rf'\b{re.escape(tech)}\b', text, re.IGNORECASE):
            found.append(tech)
    return list(set(found)) # ì¤‘ë³µ ì œê±° í›„ ë°˜í™˜

# ì„ë² ë”© ê¸°ë°˜ ë¬¸ì ìœ ì‚¬ë„ ë§¤ì¹­ (ë¬¸ë§¥ ê¸°ë°˜ ìœ ì‚¬ë„ ê·¼ì ‘ í‚¤ì›Œë“œ ì¶”ì¶œ)
def embedding_based_matching(text: str, tech_stack: list, threshold: float=0.35) -> list:
    """
    ë¬¸ì¥ ì„ë² ë”©ê³¼ ê¸°ìˆ  í‚¤ì›Œë“œ ì„ë² ë”© ê°„ ìœ ì‚¬ë„ ë¹„êµë¥¼ í†µí•´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
    """
    # ì‚¬ì „í•™ìŠµëœ ê²½ëŸ‰ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìµœì´ˆ 1íšŒ ë¡œë“œ)
    model = SentenceTransformer('all-MiniLM-L6-v2')

    # ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = sent_tokenize(text)

    # ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”© ê³„ì‚°
    sentence_embeddings = model.encode(sentences)

    # ê¸°ìˆ  ìŠ¤íƒ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ì „ì²´ ì„ë² ë”©
    keyword_embeddings = model.encode(tech_stack)

    extracted = set() # ì¤‘ë³µ ë°©ì§€

    # ê° ë¬¸ì¥ ì„ë² ë”©ë³„ë¡œ ê¸°ìˆ ìŠ¤íƒ ì„ë² ë”©ê³¼ ìœ ì‚¬ë„ ë¹„êµ
    for sentence_embedding in sentence_embeddings:
        sims = cosine_similarity([sentence_embedding], keyword_embeddings)[0]
        for tech, score in zip(tech_stack, sims):
            if score >= threshold: # ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ ê²½ìš°ë§Œ ì¶”ì¶œ
                extracted.add(tech)
    
    return list(extracted)

# ìµœì¢… íŒŒì´í”„ë¼ì¸ (ì‚¬ì „ + ì„ë² ë”©)
def extract_keywords(pdf_path: str) -> list:
    """
    ì „ì²´ ê¸°ìˆ ìŠ¤íƒ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸:
    1. PDF -> í…ìŠ¤íŠ¸ ì¶”ì¶œ
    2. ì‚¬ì „ ê¸°ë°˜ ë§¤ì¹­
    3. ì„ë² ë”© ê¸°ë°˜ ë§¤ì¹­
    4. í†µí•© í›„ ì¤‘ë³µ ì œê±°
    """
    try:
        # PDF -> í…ìŠ¤íŠ¸ ì¶”ì¶œ
        text = extract_text_from_pdf(pdf_path)

        # ì‚¬ì „ ê¸°ë°˜ ì¶”ì¶œ
        dict_match = dictionary_based_matching(text, TECH_STACK)

        # ì„ë² ë”© ê¸°ë°˜ ì¶”ì¶œ
        embed_match = embedding_based_matching(text, TECH_STACK)

        # ë‘ ê²°ê³¼ í†µí•© (ì¤‘ë³µì œê±°)
        total = list(set(dict_match + embed_match))

        return total
    except Exception as e:
        print("ğŸ”¥ğŸ”¥ğŸ”¥ [extract_keywords] ì—ëŸ¬ ë°œìƒ!")
        traceback.print_exc()
        raise e